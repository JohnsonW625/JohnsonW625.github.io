{
  "generated_at_utc": "2026-02-22T02:05:49Z",
  "query": "(all:\"large language model\" OR all:\"generative ai\")",
  "max_results": 12,
  "count": 12,
  "papers": [
    {
      "id": "http://arxiv.org/abs/2602.17655v1",
      "title": "What Language is This? Ask Your Tokenizer",
      "authors": [
        "Clara Meister",
        "Ahmetcan Yavuz",
        "Pietro Lesci",
        "Tiago Pimentel"
      ],
      "summary": "Language Identification (LID) is an important component of many multilingual natural language processing pipelines, where it facilitates corpus curation, training data analysis, and cross-lingual evaluation of large language models. Despite near-perfect performance on high-resource languages, existing systems remain brittle in low-resource and closely related language settings. We introduce UniLID, a simple and efficient LID method based on the UnigramLM tokenization algorithm, leveraging its probabilistic framing, parameter estimation technique and inference strategy. In short, we learn language-conditional unigram distributions over a shared tokenizer vocabulary but treat segmentation as a language-specific phenomenon. Our formulation is data- and compute-efficient, supports incremental addition of new languages without retraining existing models, and can naturally be integrated into existing language model tokenization pipelines. Empirical evaluations against widely used baselines, including fastText, GlotLID, and CLD3, show that UniLID achieves competitive performance on standard benchmarks, substantially improves sample efficiency in low-resource settings - surpassing 70% accuracy with as few as five labeled samples per language - and delivers large gains on fine-grained dialect identification.",
      "published": "2026-02-19T18:58:39Z",
      "updated": "2026-02-19T18:58:39Z",
      "pdf_url": "https://arxiv.org/pdf/2602.17655v1"
    },
    {
      "id": "http://arxiv.org/abs/2602.17616v1",
      "title": "Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs",
      "authors": [
        "Luke Huang",
        "Zhuoyang Zhang",
        "Qinghao Hu",
        "Shang Yang",
        "Song Han"
      ],
      "summary": "Reinforcement learning (RL) is widely used to improve large language models on reasoning tasks, and asynchronous RL training is attractive because it increases end-to-end throughput. However, for widely adopted critic-free policy-gradient methods such as REINFORCE and GRPO, high asynchrony makes the policy-gradient estimator markedly $\\textbf{higher variance}$: training on stale rollouts creates heavy-tailed importance ratios, causing a small fraction of samples to dominate updates. This amplification makes gradients noisy and learning unstable relative to matched on-policy training. Across math and general reasoning benchmarks, we find collapse is reliably predicted by effective sample size (ESS) and unstable gradient norms. Motivated by this diagnosis, we propose $\\textbf{V}$ariance $\\textbf{C}$ontrolled $\\textbf{P}$olicy $\\textbf{O}$ptimization ($\\textbf{VCPO}$), a general stabilization method for REINFORCE/GRPO-style algorithms that (i) scales learning rate based on effective sample size to dampen unreliable updates, and (ii) applies a closed-form minimum-variance baseline for the off-policy setting, avoiding an auxiliary value model and adding minimal overhead. Empirically, VCPO substantially improves robustness for asynchronous training across math, general reasoning, and tool-use tasks, outperforming a broad suite of baselines spanning masking/clipping stabilizers and algorithmic variants. This reduces long-context, multi-turn training time by 2.5$\\times$ while matching synchronous performance, demonstrating that explicit control of policy-gradient variance is key for reliable asynchronous RL at scale.",
      "published": "2026-02-19T18:40:51Z",
      "updated": "2026-02-19T18:40:51Z",
      "pdf_url": "https://arxiv.org/pdf/2602.17616v1"
    },
    {
      "id": "http://arxiv.org/abs/2505.02819v4",
      "title": "ReplaceMe: Network Simplification via Depth Pruning and Transformer Block Linearization",
      "authors": [
        "Dmitriy Shopkhoev",
        "Ammar Ali",
        "Magauiya Zhussip",
        "Valentin Malykh",
        "Stamatios Lefkimmiatis",
        "Nikos Komodakis",
        "Sergey Zagoruyko"
      ],
      "summary": "We introduce ReplaceMe, a generalized training-free depth pruning method that effectively replaces transformer blocks with a linear operation, while maintaining high performance for low compression ratios. In contrast to conventional pruning approaches that require additional training or fine-tuning, our approach requires only a small calibration dataset that is used to estimate a linear transformation, which approximates the pruned blocks. The estimated linear mapping can be seamlessly merged with the remaining transformer blocks, eliminating the need for any additional network parameters. Our experiments show that ReplaceMe consistently outperforms other training-free approaches and remains highly competitive with state-of-the-art pruning methods that involve extensive retraining/fine-tuning and architectural modifications. Applied to several large language models (LLMs), ReplaceMe achieves up to 25\\% pruning while retaining approximately 90\\% of the original model's performance on open benchmarks - without any training or healing steps, resulting in minimal computational overhead. We provide an open-source library implementing ReplaceMe alongside several state-of-the-art depth pruning techniques, available at https://github.com/mts-ai/ReplaceMe",
      "published": "2025-05-05T17:47:42Z",
      "updated": "2026-02-19T18:32:53Z",
      "pdf_url": "https://arxiv.org/pdf/2505.02819v4"
    },
    {
      "id": "http://arxiv.org/abs/2602.17608v1",
      "title": "Towards Anytime-Valid Statistical Watermarking",
      "authors": [
        "Baihe Huang",
        "Eric Xu",
        "Kannan Ramchandran",
        "Jiantao Jiao",
        "Michael I. Jordan"
      ],
      "summary": "The proliferation of Large Language Models (LLMs) necessitates efficient mechanisms to distinguish machine-generated content from human text. While statistical watermarking has emerged as a promising solution, existing methods suffer from two critical limitations: the lack of a principled approach for selecting sampling distributions and the reliance on fixed-horizon hypothesis testing, which precludes valid early stopping. In this paper, we bridge this gap by developing the first e-value-based watermarking framework, Anchored E-Watermarking, that unifies optimal sampling with anytime-valid inference. Unlike traditional approaches where optional stopping invalidates Type-I error guarantees, our framework enables valid, anytime-inference by constructing a test supermartingale for the detection process. By leveraging an anchor distribution to approximate the target model, we characterize the optimal e-value with respect to the worst-case log-growth rate and derive the optimal expected stopping time. Our theoretical claims are substantiated by simulations and evaluations on established benchmarks, showing that our framework can significantly enhance sample efficiency, reducing the average token budget required for detection by 13-15% relative to state-of-the-art baselines.",
      "published": "2026-02-19T18:32:26Z",
      "updated": "2026-02-19T18:32:26Z",
      "pdf_url": "https://arxiv.org/pdf/2602.17608v1"
    },
    {
      "id": "http://arxiv.org/abs/2509.22075v4",
      "title": "CoSpaDi: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning",
      "authors": [
        "Denis Makhov",
        "Dmitriy Shopkhoev",
        "Magauiya Zhussip",
        "Ammar Ali",
        "Stamatios Lefkimmiatis"
      ],
      "summary": "Post-training compression of large language models (LLMs) often relies on low-rank weight approximations that represent each column of the weight matrix in a shared low-dimensional subspace. This strategy is computationally efficient but the underlying constraint can be overly rigid for heterogeneous projection weights and may incur avoidable accuracy loss. We propose CoSpaDi (Compression via Sparse Dictionary Learning), a training-free framework that replaces low-rank factorization with a structured sparse decomposition in which each weight matrix is represented as a dense dictionary multiplied by a column-sparse coefficient matrix. This yields a union-of-subspaces model: the columns of the weight matrix are represented as linear combinations of different subsets of dictionary atoms, improving expressiveness at a fixed parameter budget. CoSpaDi is calibration-guided: using a small calibration set, we optimize the factorization to minimize functional reconstruction error of layer outputs rather than weight-space error. An activation-derived Gram orthonormalization reformulates this data-aware objective into a standard dictionary learning problem on transformed weights, and we support both per-layer compression and cross-layer dictionary sharing within groups of similar projections. Across Llama and Qwen model families, CoSpaDi consistently improves the accuracy--compression and perplexity--compression trade-offs over state-of-the-art SVD-based baselines and strong structured pruning baselines at 20-40\\% compression ratios. The resulting structured sparsity enables sparse--dense computation and integrates with post-training quantization of the sparse coefficients.",
      "published": "2025-09-26T08:55:09Z",
      "updated": "2026-02-19T17:30:28Z",
      "pdf_url": "https://arxiv.org/pdf/2509.22075v4"
    },
    {
      "id": "http://arxiv.org/abs/2602.17560v1",
      "title": "ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment",
      "authors": [
        "Hongjue Zhao",
        "Haosen Sun",
        "Jiangtao Kong",
        "Xiaochang Li",
        "Qineng Wang",
        "Liwei Jiang",
        "Qi Zhu",
        "Tarek Abdelzaher",
        "Yejin Choi",
        "Manling Li",
        "Huajie Shao"
      ],
      "summary": "Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \\textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \\textit{(ii)} an over-reliance on \\textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based \\textit{theoretical} framework for activation steering in LLM alignment. We show that conventional activation addition can be interpreted as a first-order approximation to the solution of an ODE. Based on this ODE perspective, identifying a steering direction becomes equivalent to designing a \\textit{barrier function} from control theory. Derived from this framework, we introduce ODESteer, a kind of ODE-based steering guided by barrier functions, which shows \\textit{empirical} advancement in LLM alignment. ODESteer identifies steering directions by defining the barrier function as the log-density ratio between positive and negative activations, and employs it to construct an ODE for \\textit{multi-step and adaptive} steering. Compared to state-of-the-art activation steering methods, ODESteer achieves consistent empirical improvements on diverse LLM alignment benchmarks, a notable $5.7\\%$ improvement over TruthfulQA, $2.5\\%$ over UltraFeedback, and $2.4\\%$ over RealToxicityPrompts. Our work establishes a principled new view of activation steering in LLM alignment by unifying its theoretical foundations via ODEs, and validating it empirically through the proposed ODESteer method.",
      "published": "2026-02-19T17:13:44Z",
      "updated": "2026-02-19T17:13:44Z",
      "pdf_url": "https://arxiv.org/pdf/2602.17560v1"
    },
    {
      "id": "http://arxiv.org/abs/2406.04220v6",
      "title": "BEADs: Bias Evaluation Across Domains",
      "authors": [
        "Shaina Raza",
        "Mizanur Rahman",
        "Michael R. Zhang"
      ],
      "summary": "Recent advances in large language models (LLMs) have substantially improved natural language processing (NLP) applications. However, these models often inherit and amplify biases present in their training data. Although several datasets exist for bias detection, most are limited to one or two NLP tasks, typically classification or evaluation and do not provide broad coverage across diverse task settings. To address this gap, we introduce the \\textbf{Bias Evaluations Across Domains} (\\textbf{B}\\texttt{EADs}) dataset, designed to support a wide range of NLP tasks, including text classification, token classification, bias quantification, and benign language generation. A key contribution of this work is a gold-standard annotation scheme that supports both evaluation and supervised training of language models. Experiments on state-of-the-art models reveal some gaps: some models exhibit systematic bias toward specific demographics, while others apply safety guardrails more strictly or inconsistently across groups. Overall, these results highlight persistent shortcomings in current models and underscore the need for comprehensive bias evaluation. Project: https://vectorinstitute.github.io/BEAD/ Data: https://huggingface.co/datasets/shainar/BEAD",
      "published": "2024-06-06T16:18:30Z",
      "updated": "2026-02-19T17:12:55Z",
      "pdf_url": "https://arxiv.org/pdf/2406.04220v6"
    },
    {
      "id": "http://arxiv.org/abs/2602.17558v1",
      "title": "RetouchIQ: MLLM Agents for Instruction-Based Image Retouching with Generalist Reward",
      "authors": [
        "Qiucheng Wu",
        "Jing Shi",
        "Simon Jenni",
        "Kushal Kafle",
        "Tianyu Wang",
        "Shiyu Chang",
        "Handong Zhao"
      ],
      "summary": "Recent advances in multimodal large language models (MLLMs) have shown great potential for extending vision-language reasoning to professional tool-based image editing, enabling intuitive and creative editing. A promising direction is to use reinforcement learning (RL) to enable MLLMs to reason about and execute optimal tool-use plans within professional image-editing software. However, training remains challenging due to the lack of reliable, verifiable reward signals that can reflect the inherently subjective nature of creative editing. In this work, we introduce RetouchIQ, a framework that performs instruction-based executable image editing through MLLM agents guided by a generalist reward model. RetouchIQ interprets user-specified editing intentions and generates corresponding, executable image adjustments, bridging high-level aesthetic goals with precise parameter control. To move beyond conventional, rule-based rewards that compute similarity against a fixed reference image using handcrafted metrics, we propose a generalist reward model, an RL fine-tuned MLLM that evaluates edited results through a set of generated metrics on a case-by-case basis. Then, the reward model provides scalar feedback through multimodal reasoning, enabling reinforcement learning with high-quality, instruction-consistent gradients. We curate an extended dataset with 190k instruction-reasoning pairs and establish a new benchmark for instruction-based image editing. Experiments show that RetouchIQ substantially improves both semantic consistency and perceptual quality over previous MLLM-based and diffusion-based editing systems. Our findings demonstrate the potential of generalist reward-driven MLLM agents as flexible, explainable, and executable assistants for professional image editing.",
      "published": "2026-02-19T17:11:59Z",
      "updated": "2026-02-19T17:11:59Z",
      "pdf_url": "https://arxiv.org/pdf/2602.17558v1"
    },
    {
      "id": "http://arxiv.org/abs/2602.17555v1",
      "title": "GraphThinker: Reinforcing Video Reasoning with Event Graph Thinking",
      "authors": [
        "Zixu Cheng",
        "Da Li",
        "Jian Hu",
        "Ziquan Liu",
        "Wei Li",
        "Shaogang Gong"
      ],
      "summary": "Video reasoning requires understanding the causal relationships between events in a video. However, such relationships are often implicit and costly to annotate manually. While existing multimodal large language models (MLLMs) often infer event relations through dense captions or video summaries for video reasoning, such modeling still lacks causal understanding. Without explicit causal structure modeling within and across video events, these models suffer from hallucinations during the video reasoning. In this work, we propose GraphThinker, a reinforcement finetuning-based method that constructs structural event-level scene graphs and enhances visual grounding to jointly reduce hallucinations in video reasoning. Specifically, we first employ an MLLM to construct an event-based video scene graph (EVSG) that explicitly models both intra- and inter-event relations, and incorporate these formed scene graphs into the MLLM as an intermediate thinking process. We also introduce a visual attention reward during reinforcement finetuning, which strengthens video grounding and further mitigates hallucinations. We evaluate GraphThinker on two datasets, RexTime and VidHalluc, where it shows superior ability to capture object and event relations with more precise event localization, reducing hallucinations in video reasoning compared to prior methods.",
      "published": "2026-02-19T17:09:30Z",
      "updated": "2026-02-19T17:09:30Z",
      "pdf_url": "https://arxiv.org/pdf/2602.17555v1"
    },
    {
      "id": "http://arxiv.org/abs/2602.17554v1",
      "title": "A Theoretical Framework for Modular Learning of Robust Generative Models",
      "authors": [
        "Corinna Cortes",
        "Mehryar Mohri",
        "Yutao Zhong"
      ],
      "summary": "Training large-scale generative models is resource-intensive and relies heavily on heuristic dataset weighting. We address two fundamental questions: Can we train Large Language Models (LLMs) modularly-combining small, domain-specific experts to match monolithic performance-and can we do so robustly for any data mixture, eliminating heuristic tuning? We present a theoretical framework for modular generative modeling where a set of pre-trained experts are combined via a gating mechanism. We define the space of normalized gating functions, $G_{1}$, and formulate the problem as a minimax game to find a single robust gate that minimizes divergence to the worst-case data mixture. We prove the existence of such a robust gate using Kakutani's fixed-point theorem and show that modularity acts as a strong regularizer, with generalization bounds scaling with the lightweight gate's complexity. Furthermore, we prove that this modular approach can theoretically outperform models retrained on aggregate data, with the gap characterized by the Jensen-Shannon Divergence. Finally, we introduce a scalable Stochastic Primal-Dual algorithm and a Structural Distillation method for efficient inference. Empirical results on synthetic and real-world datasets confirm that our modular architecture effectively mitigates gradient conflict and can robustly outperform monolithic baselines.",
      "published": "2026-02-19T17:09:13Z",
      "updated": "2026-02-19T17:09:13Z",
      "pdf_url": "https://arxiv.org/pdf/2602.17554v1"
    },
    {
      "id": "http://arxiv.org/abs/2602.17550v1",
      "title": "MASPO: Unifying Gradient Utilization, Probability Mass, and Signal Reliability for Robust and Sample-Efficient LLM Reasoning",
      "authors": [
        "Xiaoliang Fu",
        "Jiaye Lin",
        "Yangyi Fang",
        "Binbin Zheng",
        "Chaowen Hu",
        "Zekai Shao",
        "Cong Qin",
        "Lu Pan",
        "Ke Zeng",
        "Xunliang Cai"
      ],
      "summary": "Existing Reinforcement Learning with Verifiable Rewards (RLVR) algorithms, such as GRPO, rely on rigid, uniform, and symmetric trust region mechanisms that are fundamentally misaligned with the complex optimization dynamics of Large Language Models (LLMs). In this paper, we identify three critical challenges in these methods: (1) inefficient gradient utilization caused by the binary cutoff of hard clipping, (2) insensitive probability mass arising from uniform ratio constraints that ignore the token distribution, and (3) asymmetric signal reliability stemming from the disparate credit assignment ambiguity between positive and negative samples. To bridge these gaps, we propose Mass-Adaptive Soft Policy Optimization (MASPO), a unified framework designed to harmonize these three dimensions. MASPO integrates a differentiable soft Gaussian gating to maximize gradient utility, a mass-adaptive limiter to balance exploration across the probability spectrum, and an asymmetric risk controller to align update magnitudes with signal confidence. Extensive evaluations demonstrate that MASPO serves as a robust, all-in-one RLVR solution, significantly outperforming strong baselines. Our code is available at: https://anonymous.4open.science/r/ma1/README.md.",
      "published": "2026-02-19T17:05:20Z",
      "updated": "2026-02-19T17:05:20Z",
      "pdf_url": "https://arxiv.org/pdf/2602.17550v1"
    },
    {
      "id": "http://arxiv.org/abs/2602.17542v1",
      "title": "Using LLMs for Knowledge Component-level Correctness Labeling in Open-ended Coding Problems",
      "authors": [
        "Zhangqi Duan",
        "Arnav Kankaria",
        "Dhruv Kartik",
        "Andrew Lan"
      ],
      "summary": "Fine-grained skill representations, commonly referred to as knowledge components (KCs), are fundamental to many approaches in student modeling and learning analytics. However, KC-level correctness labels are rarely available in real-world datasets, especially for open-ended programming tasks where solutions typically involve multiple KCs simultaneously. Simply propagating problem-level correctness to all associated KCs obscures partial mastery and often leads to poorly fitted learning curves. To address this challenge, we propose an automated framework that leverages large language models (LLMs) to label KC-level correctness directly from student-written code. Our method assesses whether each KC is correctly applied and further introduces a temporal context-aware Code-KC mapping mechanism to better align KCs with individual student code. We evaluate the resulting KC-level correctness labels in terms of learning curve fit and predictive performance using the power law of practice and the Additive Factors Model. Experimental results show that our framework leads to learning curves that are more consistent with cognitive theory and improves predictive performance, compared to baselines. Human evaluation further demonstrates substantial agreement between LLM and expert annotations.",
      "published": "2026-02-19T16:58:34Z",
      "updated": "2026-02-19T16:58:34Z",
      "pdf_url": "https://arxiv.org/pdf/2602.17542v1"
    }
  ]
}
